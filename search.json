[{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Adversarial Random Forests","text":"calculate log-likelihoods, pass arf params lik function, along data whose likelihood wish evaluate. Note piecewise constant estimator considerably worse experiment. can compute likelihoods probability scale setting log = FALSE, may result numerical underflow. also batch argument, impact results can memory efficient large datasets. instance, rerun code batches size 50: example, used data throughout. may lead overfitting practice. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(e^{-1} \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(arf, params, iris) ll_unif <- lik(arf, params_unif, iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.3194378 -mean(ll_unif) #> [1] 5.991751 # Compute likelihood in batches of 50 ll_50 <- lik(arf, params, iris, batch = 50)  # Identical results? identical(ll, ll_50) #> [1] TRUE"},{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data synthesis","title":"Adversarial Random Forests","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set \\(n = 1000\\) simulate \\(1000\\) synthetic datapoints. Resulting data plotted side side.  default behavior adversarial_rf treat integers ordered factors, warning. makes sense , say, count data limited support (e.g., number petals plant). However, probably desired behavior integer variables. Consider diamonds dataset, price classed integer.  variable clearly treated factor 11602 levels. make sure fit continuous density price, re-class feature numeric.  Using family = 'truncnorm', distribution price now modeled truncated Gaussian mixture. Though general outline histogram looks right, find implausible values, e.g. negative prices. can overcome manually setting hard lower bound.  unnecessary sufficiently large sample sizes, however. instance, training complete diamonds dataset (\\(n = 53940\\)), observe single negative price \\(10000\\) synthetic samples (shown).","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 90.36% #> Iteration: 1, Accuracy: 38.4%  # Estimate parameters params <- forde(arf, x)  # Simulate data synth <- forge(params, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.667 -0.645 -0.913 -0.781 -0.756 ... #>  $ Y    : num  1.01 1.129 1.097 0.913 0.787 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -1.037 0.827 -0.863 0.879 -0.033 ... #>  $ Y    : num  1.014 -0.379 -0.216 1.039 0.194 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 4 4 2 3 4 2 4 4 1 ...  # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) # Check data head(diamonds) #> # A tibble: 6 × 10 #>   carat cut       color clarity depth table price     x     y     z #>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48  # View the distribution hist(diamonds$price) # How many unique prices? length(unique(diamonds$price)) #> [1] 11602 # Re-class  diamonds$price <- as.numeric(diamonds$price)  # Take a random subsample of size 1000 s_idx <- sample(1:nrow(diamonds), 1000)  # Train ARF arf <- adversarial_rf(diamonds[s_idx, ]) #> Iteration: 0, Accuracy: 96.66% #> Iteration: 1, Accuracy: 72.98% #> Iteration: 2, Accuracy: 51.13% #> Iteration: 3, Accuracy: 49.39%  # Fit density params <- forde(arf, diamonds[s_idx, ])  # Check distributional families params$meta #>     variable          class    family #>  1:    carat        numeric truncnorm #>  2:      cut ordered,factor  multinom #>  3:    color ordered,factor  multinom #>  4:  clarity ordered,factor  multinom #>  5:    depth        numeric truncnorm #>  6:    table        numeric truncnorm #>  7:    price        numeric truncnorm #>  8:        x        numeric truncnorm #>  9:        y        numeric truncnorm #> 10:        z        numeric truncnorm  # Forge data, check histogram synth <- forge(params, n_synth = 1000) hist(synth$price) # Set price minimum to zero params$cnt[variable == 'price', min := 0]  # Re-forge synth <- forge(params, n_synth = 1000) hist(synth$price)"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D (2022). arf: Adversarial random forests. https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/.","code":"@Manual{,   title = {arf: Adversarial random forests},   author = {Marvin N. Wright and David S. Watson},   year = {2022},   note = {https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/}, }"},{"path":"https://bips-hb.github.io/arf/index.html","id":"arf-adversarial-random-forests","dir":"","previous_headings":"","what":"arf: Adversarial Random Forests","title":"Adversarial random forests","text":"Adversarial random forests density estimation generative modeling","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Introduction","title":"Adversarial random forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Installation","title":"Adversarial random forests","text":"install development version GitHub using devtools, run","code":"devtools::install_github(\"bips-hb/arf\")"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Density estimation","title":"Adversarial random forests","text":"run adversarial random forest iris data, perform density estimation calculate log-likelihood data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Generative modeling","title":"Adversarial random forests","text":"generate synthetic data based iris data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"References","title":"Adversarial random forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2022). Adversarial random forests density estimation generative modeling. Preprint: https://arxiv.org/abs/2205.09435.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10,   min_node_size = 2,   delta = 0,   max_iters = 10,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real synthetic samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupservised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables treated ordered factors default. ARF passed forde, estimated distribution variables support observed factor levels (.e., output pmf, pdf). override behavior assign nonzero density intermediate values, explicitly recode features numeric. Note: convergence guaranteed finite samples. max_iter argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 83.45% #> Warning: executing %dopar% sequentially: no parallel backend registered #> Iteration: 1, Accuracy: 40.74%"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   epsilon = 0.1,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. epsilon Slack parameter empirical bounds family = \"unif\". avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor epsilon. used variable never selected splitting. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 4 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. values stored data.table, can used input various functions. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 84.69% #> Iteration: 1, Accuracy: 42.57% psi <- forde(arf, iris) head(psi) #> $cnt #>          variable  min  max f_idx       mu        sigma #>   1: Sepal.Length -Inf  Inf    12 5.155556 0.2172083957 #>   2: Sepal.Length -Inf  Inf    21 4.900000 0.1000000000 #>   3: Sepal.Length -Inf  Inf     4 4.511111 0.1452966315 #>   4: Sepal.Length -Inf 4.75     2 4.600000 0.0008211816 #>   5: Sepal.Length -Inf  Inf    18 5.200000 0.6745368782 #>  ---                                                    #> 676:  Petal.Width -Inf  Inf   168 1.475000 0.0500000000 #> 677:  Petal.Width -Inf  Inf   162 1.862500 0.0916125381 #> 678:  Petal.Width -Inf 2.35   155 2.009091 0.2553963052 #> 679:  Petal.Width -Inf  Inf   158 2.460000 0.0547722558 #> 680:  Petal.Width -Inf  Inf   154 2.000000 0.2708012802 #>  #> $cat #>      variable       val       prob f_idx #>   1:  Species    setosa 1.00000000    12 #>   2:  Species    setosa 1.00000000    21 #>   3:  Species    setosa 1.00000000     4 #>   4:  Species    setosa 1.00000000     2 #>   5:  Species    setosa 0.80000000    18 #>  ---                                     #> 198:  Species virginica 1.00000000   154 #> 199:  Species virginica 0.96969697   155 #> 200:  Species virginica 0.09090909   164 #> 201:  Species virginica 0.87500000   162 #> 202:  Species virginica 0.14285714   165 #>  #> $forest #>      f_idx tree leaf        cvg #>   1:     1    1   12 0.02000000 #>   2:     2    1   17 0.01333333 #>   3:     3    1   26 0.04000000 #>   4:     4    1   29 0.06000000 #>   5:     5    1   38 0.04666667 #>  ---                            #> 166:   166   10   68 0.16000000 #> 167:   167   10   74 0.08000000 #> 168:   168   10   79 0.02666667 #> 169:   169   10   82 0.08666667 #> 170:   170   10   85 0.01333333 #>  #> $meta #>        variable   class    family #> 1: Sepal.Length numeric truncnorm #> 2:  Sepal.Width numeric truncnorm #> 3: Petal.Length numeric truncnorm #> 4:  Petal.Width numeric truncnorm #> 5:      Species  factor  multinom #>"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(params, n_synth, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Parameters learned via forde. n_synth Number synthetic samples generate. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion coverage. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 89.6% #> Iteration: 1, Accuracy: 44.3% psi <- forde(arf, iris) x_synth <- forge(psi, n_synth = 100)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute density input data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(arf, params, x, oob = FALSE, log = TRUE, batch = NULL, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. params Parameters learned via forde. x Input data. Densities computed sample. oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities x one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes density input data according FORDE model using pre-trained ARF. sample's likelihood weighted average likelihood leaves whose split criteria satisfies. Intra-leaf densities fully factorized, since ARFs satisfy local independence criterion construction. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Estimate average log-likelihood arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 84.62% #> Iteration: 1, Accuracy: 42.91% psi <- forde(arf, iris) ll <- lik(arf, psi, iris, log = TRUE) mean(ll) #> [1] -0.9252665"}]
