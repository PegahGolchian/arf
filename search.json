[{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Package Vignette","text":"calculate log-likelihoods, pass arf params lik function, along data whose likelihood wish evaluate. Note piecewise constant estimator considerably worse experiment. can compute likelihoods probability scale setting log = FALSE, may result numerical underflow. also batch argument, impact results can memory efficient large datasets. instance, rerun code batches size 50: example, used data throughout. may lead overfitting practice. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(e^{-1} \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(arf, params, iris) ll_unif <- lik(arf, params_unif, iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.297767 -mean(ll_unif) #> [1] Inf # Compute likelihood in batches of 50 ll_50 <- lik(arf, params, iris, batch = 50)  # Identical results? identical(ll, ll_50) #> [1] TRUE"},{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data synthesis","title":"Package Vignette","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set \\(n = 1000\\) simulate \\(1000\\) synthetic datapoints. Resulting data plotted side side.  general shape clearly recognizable, even borders always crisp. can improved training data. Note default behavior adversarial_rf treat integers ordered factors, warning. makes sense , say, count data limited support (e.g., number petals plant). However, probably desired behavior integer variables. Consider diamonds dataset, price classed integer.  variable clearly treated factor 11602 levels. make sure fit continuous density price, re-class feature numeric.  Using family = 'truncnorm', distribution price now modeled truncated Gaussian mixture. Though general outline histogram looks right, find implausible values, e.g. negative prices. can overcome manually setting hard lower bound.  unnecessary sufficiently large sample sizes, however. instance, training complete diamonds dataset (\\(n = 53940\\)), observe single negative price \\(10000\\) synthetic samples (shown).","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 89.94% #> Iteration: 1, Accuracy: 36.48%  # Estimate parameters params <- forde(arf, x)  # Simulate data synth <- forge(params, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.653 -0.71 -0.679 -1.021 -0.927 ... #>  $ Y    : num  1.136 1.049 0.909 1.161 0.984 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.8256 -0.9136 -0.9267 0.0961 -0.0873 ... #>  $ Y    : num  0.988 -0.165 0.996 -0.986 0.111 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 4 1 4 3 1 3 2 4 1 ...  # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) # Check data head(diamonds) #> # A tibble: 6 × 10 #>   carat cut       color clarity depth table price     x     y     z #>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48  # View the distribution hist(diamonds$price) # How many unique prices? length(unique(diamonds$price)) #> [1] 11602 # Re-class  diamonds$price <- as.numeric(diamonds$price)  # Take a random subsample of size 1000 s_idx <- sample(1:nrow(diamonds), 1000)  # Train ARF arf <- adversarial_rf(diamonds[s_idx, ]) #> Iteration: 0, Accuracy: 96.06% #> Iteration: 1, Accuracy: 66.68% #> Iteration: 2, Accuracy: 53.08% #> Iteration: 3, Accuracy: 49.87%  # Fit density params <- forde(arf, diamonds[s_idx, ])  # Check distributional families params$meta #>     variable          class    family #>  1:    carat        numeric truncnorm #>  2:      cut ordered,factor  multinom #>  3:    color ordered,factor  multinom #>  4:  clarity ordered,factor  multinom #>  5:    depth        numeric truncnorm #>  6:    table        numeric truncnorm #>  7:    price        numeric truncnorm #>  8:        x        numeric truncnorm #>  9:        y        numeric truncnorm #> 10:        z        numeric truncnorm  # Forge data, check histogram synth <- forge(params, n_synth = 1000) hist(synth$price) # Set price minimum to zero params$cnt[variable == 'price', min := 0]  # Re-forge synth <- forge(params, n_synth = 1000) hist(synth$price)"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D (2022). arf: Adversarial Random Forests. https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/.","code":"@Manual{,   title = {arf: Adversarial Random Forests},   author = {Marvin N. Wright and David S. Watson},   year = {2022},   note = {https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/}, }"},{"path":"https://bips-hb.github.io/arf/index.html","id":"arf-adversarial-random-forests","dir":"","previous_headings":"","what":"arf: Adversarial Random Forests","title":"Adversarial Random Forests","text":"Adversarial random forests density estimation generative modeling","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Introduction","title":"Adversarial Random Forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Installation","title":"Adversarial Random Forests","text":"install development version GitHub using devtools, run","code":"devtools::install_github(\"bips-hb/arf\")"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Density estimation","title":"Adversarial Random Forests","text":"run adversarial random forest iris data, perform density estimation calculate log-likelihood data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Generative modeling","title":"Adversarial Random Forests","text":"generate synthetic data based iris data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"References","title":"Adversarial Random Forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2022). Adversarial random forests density estimation generative modeling. Preprint: https://arxiv.org/abs/2205.09435.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10L,   min_node_size = 2L,   delta = 0,   max_iters = 10L,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real synthetic samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupservised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables treated ordered factors default. ARF passed forde, estimated distribution variables support observed factor levels (.e., output pmf, pdf). override behavior assign nonzero density intermediate values, explicitly recode features numeric. Note: convergence guaranteed finite samples. max_iter argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 88.51% #> Warning: executing %dopar% sequentially: no parallel backend registered #> Iteration: 1, Accuracy: 37.5%"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   alpha = 0,   epsilon = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. alpha Optional pseudocount Laplace smoothing multinomial features. avoids zero-mass points test data fall outside support training data. Effectively parametrizes flat Dirichlet prior multinomial likelihoods. epsilon Optional slack parameter empirical bounds family = \"unif\". avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor 1 + epsilon. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 4 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. values stored data.table, can used input various functions. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 83.84% #> Iteration: 1, Accuracy: 42.81% psi <- forde(arf, iris) head(psi) #> $cnt #>          variable  min  max       mu      sigma f_idx #>   1: Sepal.Length -Inf  Inf 5.203125 0.32575904     4 #>   2: Sepal.Length -Inf 5.05 4.717647 0.23247391     3 #>   3: Sepal.Length 5.05  Inf 5.100000 0.26920569     1 #>   4: Sepal.Length -Inf 5.05 4.800000 0.26457513     5 #>   5: Sepal.Length 6.05  Inf 6.417391 0.28066832    10 #>  ---                                                  #> 668:  Petal.Width 1.35  Inf 1.825000 0.32201809   162 #> 669:  Petal.Width -Inf 1.35 1.250000 0.07071068   157 #> 670:  Petal.Width 1.35  Inf 1.525000 0.08864053   159 #> 671:  Petal.Width 1.35  Inf 2.075000 0.18322508   161 #> 672:  Petal.Width 1.35  Inf 2.060000 0.20736441   165 #>  #> $cat #>      variable        val f_idx      prob #>   1:  Species     setosa     4 0.9687500 #>   2:  Species     setosa     3 1.0000000 #>   3:  Species     setosa     1 0.5000000 #>   4:  Species     setosa     5 0.3333333 #>   5:  Species versicolor    10 0.8695652 #>  ---                                     #> 203:  Species  virginica   162 0.7500000 #> 204:  Species  virginica   160 0.7500000 #> 205:  Species  virginica   161 1.0000000 #> 206:  Species  virginica   159 0.1250000 #> 207:  Species  virginica   165 1.0000000 #>  #> $forest #>      f_idx tree leaf        cvg #>   1:     1    1   15 0.01333333 #>   2:     2    1   22 0.01333333 #>   3:     3    1   27 0.11333333 #>   4:     4    1   32 0.21333333 #>   5:     5    1   37 0.02000000 #>  ---                            #> 164:   164   10   62 0.16000000 #> 165:   165   10   66 0.03333333 #> 166:   166   10   68 0.01333333 #> 167:   167   10   71 0.07333333 #> 168:   168   10   73 0.07333333 #>  #> $meta #>        variable   class    family #> 1: Sepal.Length numeric truncnorm #> 2:  Sepal.Width numeric truncnorm #> 3: Petal.Length numeric truncnorm #> 4:  Petal.Width numeric truncnorm #> 5:      Species  factor  multinom #>  #> $input_class #> [1] \"data.frame\" #>"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(params, n_synth, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Parameters learned via forde. n_synth Number synthetic samples generate. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion coverage. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.15% #> Iteration: 1, Accuracy: 40.6% psi <- forde(arf, iris) x_synth <- forge(psi, n_synth = 100)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute density input data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(arf, params, x, oob = FALSE, log = TRUE, batch = NULL, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. params Parameters learned via forde. x Input data. Densities computed sample. oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities x one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes density input data according FORDE model using pre-trained ARF. sample's likelihood weighted average likelihood leaves whose split criteria satisfies. Intra-leaf densities fully factorized, since ARFs satisfy local independence criterion construction. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Estimate average log-likelihood arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.29% #> Iteration: 1, Accuracy: 43.96% psi <- forde(arf, iris) ll <- lik(arf, psi, iris, log = TRUE) mean(ll) #> [1] -0.6239828"}]
