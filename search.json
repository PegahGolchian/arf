[{"path":"https://bips-hb.github.io/arf/articles/density_estimation.html","id":"some-words-of-caution","dir":"Articles","previous_headings":"","what":"Some words of caution","title":"Density Estimation","text":"example, used data throughout. may lead overfitting practice. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(e^{-1} \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times. default behavior adversarial_rf treat integers ordered factors, warning. makes sense , say, count data limited support (e.g., number petals plant). However, probably desired behavior integer variables. Consider diamonds dataset, price classed integer.  variable clearly treated factor 11602 levels. make sure fit continuous density price, re-class feature numeric. Using family = 'truncnorm', distribution price now modeled truncated Gaussian mixture.","code":"# Check data head(diamonds) #> # A tibble: 6 × 10 #>   carat cut       color clarity depth table price     x     y     z #>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48  # View the distribution hist(diamonds$price) # How many unique prices? length(unique(diamonds$price)) #> [1] 11602 # Re-class  diamonds$price <- as.numeric(diamonds$price)  # Just taking first 100 observations as an example arf <- adversarial_rf(diamonds[1:100, ]) #> Iteration: 0, Accuracy: 78.79% #> Iteration: 1, Accuracy: 45.5%  # Fit density params <- forde(arf, diamonds[1:100, ])  # Check distributional families params$meta #>     variable          class    family #>  1:    carat        numeric truncnorm #>  2:      cut ordered,factor  multinom #>  3:    color ordered,factor  multinom #>  4:  clarity ordered,factor  multinom #>  5:    depth        numeric truncnorm #>  6:    table        numeric truncnorm #>  7:    price        numeric truncnorm #>  8:        x        numeric truncnorm #>  9:        y        numeric truncnorm #> 10:        z        numeric truncnorm"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D (2022). arf: Adversarial random forests. https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/.","code":"@Manual{,   title = {arf: Adversarial random forests},   author = {Marvin N. Wright and David S. Watson},   year = {2022},   note = {https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/}, }"},{"path":"https://bips-hb.github.io/arf/index.html","id":"arf-adversarial-random-forests","dir":"","previous_headings":"","what":"arf: Adversarial Random Forests","title":"Adversarial random forests","text":"Adversarial random forests density estimation generative modeling","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Introduction","title":"Adversarial random forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"Installation","title":"Adversarial random forests","text":"install development version GitHub using devtools, run","code":"devtools::install_github(\"bips-hb/arf\")"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Density estimation","title":"Adversarial random forests","text":"run adversarial random forest iris data, perform density estimation calculate log-likelihood data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"arf: Adversarial Random Forests > Examples","what":"Generative modeling","title":"Adversarial random forests","text":"generate synthetic data based iris data:","code":"arf <- adversarial_rf(iris) psi <- forde(arf, iris) forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"arf: Adversarial Random Forests","what":"References","title":"Adversarial random forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2022). Adversarial random forests density estimation generative modeling. Preprint: https://arxiv.org/abs/2205.09435.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10,   min_node_size = 2,   delta = 0,   max_iters = 10,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real synthetic samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupservised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables treated ordered factors default. ARF passed forde, estimated distribution variables support observed factor levels (.e., output pmf, pdf). override behavior assign nonzero density intermediate values, explicitly recode features numeric. Note: convergence guaranteed finite samples. max_iter argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 87.25% #> Warning: executing %dopar% sequentially: no parallel backend registered #> Iteration: 1, Accuracy: 41.55%"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   epsilon = 0.1,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. epsilon Slack parameter empirical bounds family = \"unif\". avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor epsilon. used variable never selected splitting. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 4 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. values stored data.table, can used input various functions. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 81.63% #> Iteration: 1, Accuracy: 42.14% psi <- forde(arf, iris) head(psi) #> $cnt #>          variable  min  max f_idx       mu      sigma #>   1: Sepal.Length -Inf  Inf     9 5.169697 0.28883911 #>   2: Sepal.Length -Inf  Inf    13 4.900000 0.10000000 #>   3: Sepal.Length -Inf  Inf    14 4.680000 0.21679483 #>   4: Sepal.Length -Inf  Inf    11 4.800000 0.14142136 #>   5: Sepal.Length -Inf 4.65     8 4.433333 0.05773503 #>  ---                                                  #> 772:  Petal.Width -Inf  Inf   188 1.716667 0.24832774 #> 773:  Petal.Width -Inf  Inf   194 2.233333 0.37859389 #> 774:  Petal.Width -Inf  Inf   190 2.091667 0.24301846 #> 775:  Petal.Width -Inf  Inf   193 2.010000 0.25582112 #> 776:  Petal.Width -Inf  Inf   189 2.166667 0.32145503 #>  #> $cat #>      variable       val      prob f_idx #>   1:  Species    setosa 1.0000000     9 #>   2:  Species    setosa 1.0000000    13 #>   3:  Species    setosa 1.0000000    14 #>   4:  Species    setosa 1.0000000    11 #>   5:  Species    setosa 1.0000000     8 #>  ---                                    #> 222:  Species virginica 1.0000000   190 #> 223:  Species virginica 0.2000000   180 #> 224:  Species virginica 1.0000000   193 #> 225:  Species virginica 1.0000000   189 #> 226:  Species virginica 0.1666667   185 #>  #> $forest #>      f_idx tree leaf        cvg #>   1:     1    1   21 0.02666667 #>   2:     2    1   25 0.04666667 #>   3:     3    1   31 0.03333333 #>   4:     4    1   34 0.02666667 #>   5:     5    1   35 0.01333333 #>  ---                            #> 190:   190   10   77 0.16000000 #> 191:   191   10   78 0.02000000 #> 192:   192   10   79 0.05333333 #> 193:   193   10   81 0.06666667 #> 194:   194   10   83 0.02000000 #>  #> $meta #>        variable   class    family #> 1: Sepal.Length numeric truncnorm #> 2:  Sepal.Width numeric truncnorm #> 3: Petal.Length numeric truncnorm #> 4:  Petal.Width numeric truncnorm #> 5:      Species  factor  multinom #>"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(params, n_synth, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Parameters learned via forde. n_synth Number synthetic samples generate. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion coverage. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 85.81% #> Iteration: 1, Accuracy: 39.46% psi <- forde(arf, iris) x_synth <- forge(psi, n_synth = 100)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute density input data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(arf, params, x, oob = FALSE, log = TRUE, batch = NULL, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. params Parameters learned via forde. x Input data. Densities computed sample. oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities x one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes density input data according FORDE model using pre-trained ARF. sample's likelihood weighted average likelihood leaves whose split criteria satisfies. Intra-leaf densities fully factorized, since ARFs satisfy local independence criterion construction. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Estimate average log-likelihood arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 90.64% #> Iteration: 1, Accuracy: 41.14% psi <- forde(arf, iris) ll <- lik(arf, psi, iris, log = TRUE) mean(ll) #> [1] -0.847309"}]
