[{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"adversarial-training","dir":"Articles","previous_headings":"","what":"Adversarial Training","title":"Package Vignette","text":"ARF algorithm iterative procedure. first instance, generate synthetic data independently sampling marginals feature training random forest (RF) distinguish original synthetic samples. accuracy greater \\(0.5 + \\delta\\) (delta user-controlled tolerance parameter, generally set 0), create new dataset sampling marginals within leaf training another RF classifier. procedure repeats original synthetic samples reliably distinguished. default verbose = TRUE, algorithm print accuracy iteration. printouts can turned setting verbose = FALSE. Accuracy still stored within arf object, can evaluate convergence fact. warning appears just per session. can suppressed setting parallel = FALSE registering parallel backend ().  find quick drop accuracy following resampling procedure, desired. ARF converged, resulting splits form fully factorized leaves, .e. subregions feature space variables locally independent. ARF convergence almost surely guaranteed \\(n \\rightarrow \\infty\\) (see Watson et al., 2022, Thm. 1). However, implications finite sample performance. practice, often find adversarial training completes just one two rounds, may hold datasets. avoid infinite loops, users can increase slack parameter delta set max_iters argument (default = 10). addition failsafes, adversarial_rf uses early stopping default (early_stop = TRUE), terminates training factorization improve one round next. recommended, since discriminator accuracy rarely falls much lower increased. density estimation tasks, recommend increasing default number trees. generally use 100 experiments, though may suboptimal datasets. Likelihood estimates sensitive parameter certain threshold, larger models incur extra costs time memory. can speed computations registering parallel backend, case ARF training distributed across cores using ranger package. Much like ranger, default behavior adversarial_rf compute parallel possible. exactly done varies across operating systems. following code works Unix machines. Windows requires different setup. either case, can now execute parallel. result object class ranger, can input downstream functions.","code":"# Load libraries library(arf) library(data.table) library(ggplot2)  # Set seed set.seed(123, \"L'Ecuyer-CMRG\")  # Train ARF arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 86.53% #> Iteration: 1, Accuracy: 44.44% #> Warning: executing %dopar% sequentially: no parallel backend registered # Train ARF with no printouts arf <- adversarial_rf(iris, verbose = FALSE)  # Plot accuracy against iterations (model converges when accuracy <= 0.5) tmp <- data.frame('acc' = arf$acc, 'iter' = seq_len(length(arf$acc))) ggplot(tmp, aes(iter, acc)) +    geom_point() +    geom_path() +   geom_hline(yintercept = 0.5, linetype = 'dashed', color = 'red') # Register cores - Unix library(doParallel) registerDoParallel(cores = 2) # Register cores - Windows library(doParallel) cl <- makeCluster(2) registerDoParallel(cl) # Rerun ARF, now in parallel and with more trees arf <- adversarial_rf(iris, num_trees = 100) #> Iteration: 0, Accuracy: 92.33% #> Iteration: 1, Accuracy: 28%"},{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"parameter-learning","dir":"Articles","previous_headings":"","what":"Parameter Learning","title":"Package Vignette","text":"next step learn leaf distribution parameters using forests density estimation (FORDE). function calculates coverage, bounds, pdf/pmf parameters every variable every leaf. can expensive computation large datasets, requires \\(\\mathcal{O}\\big(B \\cdot d \\cdot n \\cdot \\log(n)\\big)\\) operations, \\(B\\) number trees, \\(d\\) data dimensionality, \\(n\\) sample size. , process parallelized default. Default behavior use truncated normal distribution continuous data (boundaries given tree’s split parameters) multinomial distribution categorical data. find produces stable results wide range settings. can also use uniform distribution continuous features setting family = 'unif', thereby instantiating piecewise constant density estimator. method tends perform poorly practice, recommend . option implemented primarily benchmarking purposes. Alternative families, e.g. truncated Poisson beta distributions, may useful certain problems. Future releases expand range options family argument. alpha epsilon arguments allow optional regularization multinomial uniform distributions, respectively. help prevent zero likelihood samples test data fall outside support training data. former pseudocount parameter applies Laplace smoothing within leaves, preventing unobserved values assigned zero probability unless splits explicitly rule . words, impose flat Dirichlet prior \\(\\text{Dir}(\\alpha)\\) report posterior probabilities rather maximum likelihood estimates. latter slack parameter empirical bounds expands estimated extrema continuous features factor \\(1 + \\epsilon\\). Compare results original probability estimates Species variable obtained adding pseudocount \\(\\alpha = 0.1\\). Laplace smoothing, extreme probabilities occur splits explicitly demand . Otherwise, values shrink toward uniform prior. Note two data tables may exactly rows, omit zero probability events conserve memory. However, can verify probabilities sum unity leaf-variable combination. forde function outputs list length 5, entries (1) continuous features; (2) categorical features; (3) leaf parameters; (4) variable metadata; (5) data input class. parameters can used variety downstream tasks, likelihood estimation data synthesis.","code":"# Compute leaf and distribution parameters params <- forde(arf, iris) # Recompute with uniform density params_unif <- forde(arf, iris, family = 'unif') # Recompute with additive smoothing params_alpha <- forde(arf, iris, alpha = 0.1)  # Compare results head(params$cat) #>    f_idx variable        val      prob #> 1:     1  Species  virginica 1.0000000 #> 2:     2  Species     setosa 1.0000000 #> 3:     3  Species versicolor 0.6000000 #> 4:     3  Species  virginica 0.4000000 #> 5:     4  Species     setosa 1.0000000 #> 6:     5  Species versicolor 0.3333333 head(params_alpha$cat) #>    f_idx variable        val        prob #> 1:     1  Species versicolor 0.008849558 #> 2:     1  Species  virginica 0.982300885 #> 3:     1  Species     setosa 0.008849558 #> 4:     2  Species versicolor 0.008849558 #> 5:     2  Species  virginica 0.008849558 #> 6:     2  Species     setosa 0.982300885 # Sum probabilities summary(params$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 summary(params_alpha$cat[, sum(prob), by = .(f_idx, variable)]$V1) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>       1       1       1       1       1       1 params #> $cnt #>       f_idx     variable  min  max       mu      sigma #>    1:     1 Petal.Length -Inf  Inf 6.336364 0.35006493 #>    2:     1  Petal.Width -Inf  Inf 2.045455 0.26594600 #>    3:     1 Sepal.Length 7.15  Inf 7.509091 0.25477263 #>    4:     1  Sepal.Width -Inf  Inf 3.136364 0.41538591 #>    5:     2 Petal.Length -Inf 1.35 1.236364 0.10269106 #>   ---                                                  #> 7396:  1849  Sepal.Width 2.95  Inf 3.000000 0.01869639 #> 7397:  1850 Petal.Length 5.05  Inf 5.366667 0.23094011 #> 7398:  1850  Petal.Width -Inf 1.90 1.800000 0.70201186 #> 7399:  1850 Sepal.Length -Inf 6.75 6.266667 0.32145503 #> 7400:  1850  Sepal.Width 2.95  Inf 3.033333 0.05773503 #>  #> $cat #>       f_idx variable        val prob #>    1:     1  Species  virginica  1.0 #>    2:     2  Species     setosa  1.0 #>    3:     3  Species versicolor  0.6 #>    4:     3  Species  virginica  0.4 #>    5:     4  Species     setosa  1.0 #>   ---                                #> 2341:  1846  Species  virginica  1.0 #> 2342:  1847  Species versicolor  1.0 #> 2343:  1848  Species versicolor  1.0 #> 2344:  1849  Species  virginica  1.0 #> 2345:  1850  Species  virginica  1.0 #>  #> $forest #>       f_idx tree leaf        cvg #>    1:     1    1   17 0.07333333 #>    2:     2    1   18 0.07333333 #>    3:     3    1   27 0.03333333 #>    4:     4    1   30 0.06000000 #>    5:     5    1   33 0.02000000 #>   ---                            #> 1846:  1846  100   67 0.07333333 #> 1847:  1847  100   71 0.05333333 #> 1848:  1848  100   74 0.03333333 #> 1849:  1849  100   76 0.01333333 #> 1850:  1850  100   77 0.02000000 #>  #> $meta #>        variable   class    family #> 1: Sepal.Length numeric truncnorm #> 2:  Sepal.Width numeric truncnorm #> 3: Petal.Length numeric truncnorm #> 4:  Petal.Width numeric truncnorm #> 5:      Species  factor  multinom #>  #> $input_class #> [1] \"data.frame\""},{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"likelihood-estimation","dir":"Articles","previous_headings":"","what":"Likelihood Estimation","title":"Package Vignette","text":"calculate log-likelihoods, pass arf params lik function, along data whose likelihood wish evaluate. Note piecewise constant estimator considerably worse experiment. can compute likelihoods probability scale setting log = FALSE, may result numerical underflow. also batch argument, impact results can memory efficient large datasets. instance, rerun code batches size 50: example, used data throughout. may lead overfitting. sufficient data, preferable use training set adversarial_rf, validation set forde, test set lik. Alternatively, can set oob argument TRUE either latter two functions, case computations performed --bag (OOB) data. samples randomly excluded given tree due bootstrapping subroutine RF classifier. Note works dataset x passed forde lik one used train arf. Recall sample’s probability excluded single tree \\(e^{-1} \\approx 0.368\\). using oob = TRUE, sure include enough trees every observation likely OOB least times.","code":"# Compute likelihood under truncated normal and uniform distributions ll <- lik(arf, params, iris) ll_unif <- lik(arf, params_unif, iris)  # Compare average negative log-likelihood (lower is better) -mean(ll) #> [1] 0.3133987 -mean(ll_unif) #> [1] 3.805665 # Compute likelihood in batches of 50 ll_50 <- lik(arf, params, iris, batch = 50)  # Identical results? identical(ll, ll_50) #> [1] TRUE"},{"path":"https://bips-hb.github.io/arf/articles/vignette.html","id":"data-synthesis","dir":"Articles","previous_headings":"","what":"Data synthesis","title":"Package Vignette","text":"experiment, use smiley simulation mlbench package, allows easy visual assessment. draw training set \\(n = 1000\\) simulate \\(1000\\) synthetic datapoints. Resulting data plotted side side.  general shape clearly recognizable, even stray samples evident borders always crisp. can improved training data. Note default behavior adversarial_rf treat integers ordered factors, warning. makes sense , say, count data limited support (e.g., number petals flower). However, probably desired behavior integer variables. Consider diamonds dataset, price classed integer.  variable clearly treated factor 11602 levels. make sure fit continuous density price, re-class feature numeric.  Using family = 'truncnorm', distribution price now modeled truncated Gaussian mixture. Though general outline histogram looks right, find implausible values, e.g. negative prices. can overcome manually setting hard lower bound.  Alternatively, retrain log transformation.  may unnecessary sufficiently large sample sizes. instance, negative prices exceedingly rare training complete diamonds dataset (\\(n = 53940\\)). However, natural constraints known priori, can easily incorporated params.","code":"# Simulate training data library(mlbench) x <- mlbench.smiley(1000) x <- data.frame(x$x, x$classes) colnames(x) <- c('X', 'Y', 'Class')  # Fit ARF arf <- adversarial_rf(x, mtry = 2) #> Iteration: 0, Accuracy: 90.74% #> Iteration: 1, Accuracy: 37.73%  # Estimate parameters params <- forde(arf, x)  # Simulate data synth <- forge(params, n_synth = 1000)  # Compare structure str(x) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.87 -0.7 -0.827 -0.905 -0.751 ... #>  $ Y    : num  1.032 0.778 1.068 1.137 1.083 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ... str(synth) #> 'data.frame':    1000 obs. of  3 variables: #>  $ X    : num  -0.878648 -0.847801 -0.488739 0.76994 0.000893 ... #>  $ Y    : num  0.862 1.052 -0.786 0.871 0.693 ... #>  $ Class: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 4 2 3 3 3 3 2 4 ...  # Put it all together x$Data <- 'Original' synth$Data <- 'Synthetic' df <- rbind(x, synth)  # Plot results ggplot(df, aes(X, Y, color = Class, shape = Class)) +    geom_point() +    facet_wrap(~ Data) # Check data head(diamonds) #> # A tibble: 6 × 10 #>   carat cut       color clarity depth table price     x     y     z #>   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl> #> 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43 #> 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31 #> 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31 #> 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63 #> 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75 #> 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48  # View the distribution hist(diamonds$price) # How many unique prices? length(unique(diamonds$price)) #> [1] 11602 # Re-class  diamonds$price <- as.numeric(diamonds$price)  # Take a random subsample of size 2000 s_idx <- sample(1:nrow(diamonds), 2000)  # Train ARF arf <- adversarial_rf(diamonds[s_idx, ]) #> Iteration: 0, Accuracy: 97.04% #> Iteration: 1, Accuracy: 72.16% #> Iteration: 2, Accuracy: 51.72% #> Iteration: 3, Accuracy: 50.49% #> Iteration: 4, Accuracy: 48.06%  # Estimate parameters params <- forde(arf, diamonds[s_idx, ])  # Check distributional families params$meta #>     variable          class    family #>  1:    carat        numeric truncnorm #>  2:      cut ordered,factor  multinom #>  3:    color ordered,factor  multinom #>  4:  clarity ordered,factor  multinom #>  5:    depth        numeric truncnorm #>  6:    table        numeric truncnorm #>  7:    price        numeric truncnorm #>  8:        x        numeric truncnorm #>  9:        y        numeric truncnorm #> 10:        z        numeric truncnorm  # Forge data, check histogram synth <- forge(params, n_synth = 1000) hist(synth$price) # Set price minimum to empirical lower bound params$cnt[variable == 'price', min := min(diamonds$price)]  # Re-forge, check histogram synth <- forge(params, n_synth = 1000) hist(synth$price) # Transform price variable tmp <- as.data.table(diamonds[s_idx, ]) tmp[, price := log(price)]  # Retrain ARF arf <- adversarial_rf(tmp) #> Iteration: 0, Accuracy: 96.49% #> Iteration: 1, Accuracy: 73.95% #> Iteration: 2, Accuracy: 52.15% #> Iteration: 3, Accuracy: 51.04% #> Iteration: 4, Accuracy: 50.69% #> Iteration: 5, Accuracy: 48.4%  # Estimate parameters params <- forde(arf, tmp)  # Forge, check histogram synth <- forge(params, n_synth = 1000) hist(exp(synth$price))"},{"path":"https://bips-hb.github.io/arf/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marvin N. Wright. Author, maintainer. David S. Watson. Author.","code":""},{"path":"https://bips-hb.github.io/arf/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Wright M, Watson D (2023). arf: Adversarial Random Forests. https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/.","code":"@Manual{,   title = {arf: Adversarial Random Forests},   author = {Marvin N. Wright and David S. Watson},   year = {2023},   note = {https://github.com/bips-hb/arf, https://bips-hb.github.io/arf/}, }"},{"path":[]},{"path":"https://bips-hb.github.io/arf/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Adversarial Random Forests","text":"Adversarial random forests (ARFs) recursively partition data fully factorized leaves, features jointly independent. procedure iterative, alternating rounds generation discrimination. Data become increasingly realistic round, original synthetic samples can longer reliably distinguished. useful several unsupervised learning tasks, density estimation data synthesis. Methods implemented package. ARFs naturally handle unstructured data mixed continuous categorical covariates. inherit many benefits RFs, including speed, flexibility, solid performance default parameters.","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Adversarial Random Forests","text":"arf package available CRAN: install development version GitHub using devtools, run:","code":"install.packages(\"arf\") devtools::install_github(\"bips-hb/arf\")"},{"path":"https://bips-hb.github.io/arf/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Adversarial Random Forests","text":"Using Fisher’s iris dataset, train ARF estimate distribution parameters:","code":"# Train the ARF arf <- adversarial_rf(iris)  # Estimate distribution parameters psi <- forde(arf, iris)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"density-estimation","dir":"","previous_headings":"Examples","what":"Density estimation","title":"Adversarial Random Forests","text":"estimate log-likelihoods:","code":"mean(lik(arf, psi, iris))"},{"path":"https://bips-hb.github.io/arf/index.html","id":"generative-modeling","dir":"","previous_headings":"Examples","what":"Generative modeling","title":"Adversarial Random Forests","text":"generate 100 synthetic samples: detailed examples, see package vignette.","code":"forge(psi, 100)"},{"path":"https://bips-hb.github.io/arf/index.html","id":"other-distributions","dir":"","previous_headings":"","what":"Other distributions","title":"Adversarial Random Forests","text":"Python implementation ARF, arfpy, available PyPI. development version, see .","code":""},{"path":"https://bips-hb.github.io/arf/index.html","id":"references","dir":"","previous_headings":"","what":"References","title":"Adversarial Random Forests","text":"Watson, D. S., Blesch, K., Kapar, J. & Wright, M. N. (2023). Adversarial random forests density estimation generative modeling. Proceedings 26th International Conference Artificial Intelligence Statistics. Link .","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":null,"dir":"Reference","previous_headings":"","what":"Adversarial Random Forests — adversarial_rf","title":"Adversarial Random Forests — adversarial_rf","text":"Implements adversarial random forest learn independence-inducing splits.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"adversarial_rf(   x,   num_trees = 10L,   min_node_size = 2L,   delta = 0,   max_iters = 10L,   early_stop = TRUE,   verbose = TRUE,   parallel = TRUE,   ... )"},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adversarial Random Forests — adversarial_rf","text":"x Input data. Integer variables recoded ordered factors warning. See Details. num_trees Number trees grow forest. default works well generative modeling tasks, increased likelihood estimation. See Details. min_node_size Minimal number real data samples leaf nodes. delta Tolerance parameter. Algorithm converges OOB accuracy < 0.5 + delta. max_iters Maximum iterations adversarial loop. early_stop Terminate loop performance fails improve one round next? verbose Print discriminator accuracy round? parallel Compute parallel? Must register backend beforehand, e.g. via doParallel. ... Extra parameters passed ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Adversarial Random Forests — adversarial_rf","text":"random forest object class ranger.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Adversarial Random Forests — adversarial_rf","text":"adversarial random forest (ARF) algorithm partitions data fully factorized leaves features jointly independent. ARFs trained iteratively, alternating rounds generation discrimination. first instance, synthetic data generated via independent bootstraps feature, RF classifier trained distinguish real synthetic samples. subsequent rounds, synthetic data generated separately leaf, using splits previous forest. creates increasingly realistic data satisfies local independence construction. algorithm converges RF reliably distinguish two classes, .e. OOB accuracy falls 0.5 + delta. ARFs useful several unsupservised learning tasks, density estimation (see forde) data synthesis (see forge). former, recommend increasing number trees improved performance (typically order 100-1000 depending sample size). Integer variables treated ordered factors default. ARF passed forde, estimated distribution variables support observed factor levels (.e., output pmf, pdf). override behavior assign nonzero density intermediate values, explicitly recode features numeric. Note: convergence guaranteed finite samples. max_iter argument sets upper bound number training rounds. Similar results may attained increasing delta. Even single round can often give good performance, data strong complex dependencies may require iterations. default early_stop = TRUE, adversarial loop terminates performance improve one round next, case training may pointless.","code":""},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Adversarial Random Forests — adversarial_rf","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/adversarial_rf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Adversarial Random Forests — adversarial_rf","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 85.08% #> Iteration: 1, Accuracy: 38.98% #> Warning: executing %dopar% sequentially: no parallel backend registered"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":null,"dir":"Reference","previous_headings":"","what":"Adaptive column renaming — col_rename","title":"Adaptive column renaming — col_rename","text":"function renames columns case input data.frame includes colnames required internal functions (e.g., \"y\").","code":""},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Adaptive column renaming — col_rename","text":"","code":"col_rename(df, old_name)"},{"path":"https://bips-hb.github.io/arf/reference/col_rename.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Adaptive column renaming — col_rename","text":"df Input data.frame. old_name Name column renamed.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Density Estimation — forde","title":"Forests for Density Estimation — forde","text":"Uses pre-trained ARF model estimate leaf distribution parameters.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Density Estimation — forde","text":"","code":"forde(   arf,   x,   oob = FALSE,   family = \"truncnorm\",   alpha = 0,   epsilon = 0,   parallel = TRUE )"},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Density Estimation — forde","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. x Training data estimating parameters. oob use --bag samples parameter estimation? TRUE, x must dataset used train arf. family Distribution use density estimation continuous features. Current options include truncated normal (default family = \"truncnorm\") uniform (family = \"unif\"). See Details. alpha Optional pseudocount Laplace smoothing categorical features. avoids zero-mass points test data fall outside support training data. Effectively parametrizes flat Dirichlet prior multinomial likelihoods. epsilon Optional slack parameter empirical bounds family = \"unif\". avoids zero-density points test data fall outside support training data. gap lower upper bounds expanded factor 1 + epsilon. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Density Estimation — forde","text":"list 5 elements: (1) parameters continuous data; (2) parameters discrete data; (3) leaf indices coverage; (4) metadata variables; (5) data input class. list used estimating likelihoods lik generating data forge.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Density Estimation — forde","text":"forde extracts leaf parameters pretrained forest learns distribution parameters data within leaf. former includes coverage (proportion data falling leaf) split criteria. latter includes proportions categorical features mean/variance continuous features. values stored data.table, can used input various functions. Currently, forde provides support limited number distributional families: truncated normal uniform continuous data, multinomial discrete data. Future releases accommodate larger set options. Though forde designed take adversarial random forest input, function's first argument can principle object class ranger. allows users test performance alternative pipelines (e.g., supervised forest input). also requirement x data used fit arf, unless oob = TRUE. fact, using another dataset may protect overfitting. connects Wager & Athey's (2018) notion \"honest trees\".","code":""},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Density Estimation — forde","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435. Wager, S. & Athey, S. (2018). Estimation inference heterogeneous treatment effects using random forests. J. . Stat. Assoc., 113(523): 1228-1242.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forde.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Density Estimation — forde","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 85.19% #> Iteration: 1, Accuracy: 46.96% psi <- forde(arf, iris) head(psi) #> $cnt #>      f_idx     variable  min  max       mu      sigma #>   1:     1 Petal.Length 6.05  Inf 6.450000 0.33316662 #>   2:     1  Petal.Width -Inf  Inf 2.066667 0.20655911 #>   3:     1 Sepal.Length -Inf  Inf 7.566667 0.17511901 #>   4:     1  Sepal.Width -Inf 3.60 2.850000 0.15165751 #>   5:     2 Petal.Length 6.05  Inf 6.400000 0.30000000 #>  ---                                                  #> 748:   187  Sepal.Width 2.90 3.35 3.118182 0.11677484 #> 749:   188 Petal.Length -Inf 5.25 5.150000 0.05773503 #> 750:   188  Petal.Width 1.55  Inf 2.150000 0.17320508 #> 751:   188 Sepal.Length 6.25 7.65 6.650000 0.19148542 #> 752:   188  Sepal.Width 2.90 3.35 3.075000 0.09574271 #>  #> $cat #>      f_idx variable        val      prob #>   1:     1  Species  virginica 1.0000000 #>   2:     2  Species  virginica 1.0000000 #>   3:     3  Species versicolor 0.6666667 #>   4:     3  Species  virginica 0.3333333 #>   5:     4  Species versicolor 0.5000000 #>  ---                                     #> 218:   184  Species versicolor 1.0000000 #> 219:   185  Species  virginica 1.0000000 #> 220:   186  Species  virginica 1.0000000 #> 221:   187  Species  virginica 1.0000000 #> 222:   188  Species  virginica 1.0000000 #>  #> $forest #>      f_idx tree leaf        cvg #>   1:     1    1    6 0.04000000 #>   2:     2    1    7 0.02000000 #>   3:     3    1   28 0.02000000 #>   4:     4    1   29 0.01333333 #>   5:     5    1   33 0.04000000 #>  ---                            #> 184:   184   10   66 0.03333333 #> 185:   185   10   68 0.09333333 #> 186:   186   10   70 0.02000000 #> 187:   187   10   73 0.07333333 #> 188:   188   10   74 0.02666667 #>  #> $meta #>        variable   class    family #> 1: Sepal.Length numeric truncnorm #> 2:  Sepal.Width numeric truncnorm #> 3: Petal.Length numeric truncnorm #> 4:  Petal.Width numeric truncnorm #> 5:      Species  factor  multinom #>  #> $input_class #> [1] \"data.frame\" #>"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":null,"dir":"Reference","previous_headings":"","what":"Forests for Generative Modeling — forge","title":"Forests for Generative Modeling — forge","text":"Uses pre-trained FORDE model simulate synthetic data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forests for Generative Modeling — forge","text":"","code":"forge(params, n_synth, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forests for Generative Modeling — forge","text":"params Parameters learned via forde. n_synth Number synthetic samples generate. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forests for Generative Modeling — forge","text":"dataset n_synth synthetic samples.","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forests for Generative Modeling — forge","text":"forge simulates synthetic dataset n_synth samples. First, leaves sampled proportion coverage. , feature sampled independently within leaf according probability mass density function learned forde. create realistic data long adversarial RF used previous step satisfies local independence criterion. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Forests for Generative Modeling — forge","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/forge.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Forests for Generative Modeling — forge","text":"","code":"arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 84.28% #> Iteration: 1, Accuracy: 43.24% psi <- forde(arf, iris) x_synth <- forge(psi, n_synth = 100)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":null,"dir":"Reference","previous_headings":"","what":"Likelihood Estimation — lik","title":"Likelihood Estimation — lik","text":"Compute density input data.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Likelihood Estimation — lik","text":"","code":"lik(arf, params, x, oob = FALSE, log = TRUE, batch = NULL, parallel = TRUE)"},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Likelihood Estimation — lik","text":"arf Pre-trained adversarial_rf. Alternatively, object class ranger. params Parameters learned via forde. x Input data. Densities computed sample. oob use --bag leaves likelihood estimation? TRUE, x must dataset used train arf. log Return likelihoods log scale? Recommended prevent underflow. batch Batch size. default compute densities x one round, always fastest option memory allows. However, large samples many trees, can memory efficient split data batches. impact results. parallel Compute parallel? Must register backend beforehand, e.g. via doParallel.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Likelihood Estimation — lik","text":"vector likelihoods, optionally log scale.","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Likelihood Estimation — lik","text":"function computes density input data according FORDE model using pre-trained ARF. sample's likelihood weighted average likelihood leaves whose split criteria satisfies. Intra-leaf densities fully factorized, since ARFs satisfy local independence criterion construction. See Watson et al. (2022).","code":""},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Likelihood Estimation — lik","text":"Watson, D., Blesch, K., Kapar, J., & Wright, M. (2022). Adversarial random forests density estimation generative modeling. arXiv preprint, 2205.09435.","code":""},{"path":[]},{"path":"https://bips-hb.github.io/arf/reference/lik.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Likelihood Estimation — lik","text":"","code":"# Estimate average log-likelihood arf <- adversarial_rf(iris) #> Iteration: 0, Accuracy: 87.76% #> Iteration: 1, Accuracy: 43% psi <- forde(arf, iris) ll <- lik(arf, psi, iris, log = TRUE) mean(ll) #> [1] -0.8971978"},{"path":"https://bips-hb.github.io/arf/news/index.html","id":"arf-013","dir":"Changelog","previous_headings":"","what":"arf 0.1.3","title":"arf 0.1.3","text":"CRAN release: 2023-02-06 Speed boost adversarial resampling step Early stopping option adversarial training alpha parameter regularizing multinomial distributions forde Unified treatment colnames internal semantics (y, obs, tree, leaf)","code":""}]
